{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db30617e",
   "metadata": {},
   "source": [
    "# CMAPSS — RNN / LSTM / GRU con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7cad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version reported by torch: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    import torch\n",
    "    print('PyTorch', torch.__version__)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA version reported by torch:', torch.version.cuda)\n",
    "        print('GPU count:', torch.cuda.device_count())\n",
    "        print('GPU name:', torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print('PyTorch import error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = r\"C:\\Users\\David\\Documents\\Master-Big-Data-Data-Sciencee-e-Inteligencia-Artificial\\TFM\\AeroGPT\\data\\CMAPSS\"\n",
    "RAW_PATH = os.path.join(BASE_PATH, \"raw\")\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"models_CV\")\n",
    "FIG_PATH = os.path.join(BASE_PATH, \"figures_CV\")\n",
    "METRICS_PATH = os.path.join(BASE_PATH, \"metrics_CV\")\n",
    "REPORTS_PATH = os.path.join(BASE_PATH, \"reports_CV\")\n",
    "\n",
    "for p in [MODEL_PATH, FIG_PATH, METRICS_PATH, REPORTS_PATH]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca712ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, joblib, matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27746715",
   "metadata": {},
   "source": [
    "## 3) Carga y preprocesado\n",
    "\n",
    "Funciones para cargar los archivos CMAPSS, añadir RUL, comprobar nulos, eliminar columnas constantes, escalar y crear secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64fcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fd_dataset(fd, raw_path=RAW_PATH):\n",
    "    col_names = ['unit_nr','time_cycles','setting_1','setting_2','setting_3'] + [f's_{i}' for i in range(1,22)]\n",
    "    train_file = os.path.join(raw_path, f\"train_{fd}.txt\")\n",
    "    test_file  = os.path.join(raw_path, f\"test_{fd}.txt\")\n",
    "    rul_file   = os.path.join(raw_path, f\"RUL_{fd}.txt\")\n",
    "    train_df = pd.read_csv(train_file, sep='\\s+', header=None, names=col_names)\n",
    "    test_df  = pd.read_csv(test_file,  sep='\\s+', header=None, names=col_names)\n",
    "    rul_df   = pd.read_csv(rul_file,  sep='\\s+', header=None, names=['RUL'])\n",
    "    return train_df, test_df, rul_df\n",
    "\n",
    "def add_rul(train_df):\n",
    "    max_cycles = train_df.groupby('unit_nr')['time_cycles'].max().reset_index()\n",
    "    max_cycles.columns = ['unit_nr','max_cycle']\n",
    "    df = train_df.merge(max_cycles, on='unit_nr', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['time_cycles']\n",
    "    return df.drop(columns=['max_cycle'])\n",
    "\n",
    "def check_and_report_raw(train_df, test_df, rul_df, fd, reports_path=REPORTS_PATH):\n",
    "    lines = []\n",
    "    lines.append('=== Nulos train ===')\n",
    "    lines.append(str(train_df.isnull().sum()[train_df.isnull().sum()>0]))\n",
    "    lines.append('=== Nulos test ===')\n",
    "    lines.append(str(test_df.isnull().sum()[test_df.isnull().sum()>0]))\n",
    "    n_units_test = test_df['unit_nr'].nunique()\n",
    "    lines.append(f'Unidades test: {n_units_test}, RUL rows: {len(rul_df)}')\n",
    "    lines.append(f'Filas train: {len(train_df)}, filas test: {len(test_df)}')\n",
    "    report_file = os.path.join(reports_path, f'raw_check_{fd}.txt')\n",
    "    with open(report_file,'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    print('\\n'.join(lines))\n",
    "    return report_file\n",
    "\n",
    "def remove_constant_columns(train_df, test_df, feature_cols):\n",
    "    dropped = []\n",
    "    for c in feature_cols:\n",
    "        combined = pd.concat([train_df[c], test_df[c]])\n",
    "        if combined.nunique() <= 1 or np.isclose(combined.std(), 0.0):\n",
    "            dropped.append(c)\n",
    "    if dropped:\n",
    "        print('Dropped const cols:', dropped)\n",
    "        train_df = train_df.drop(columns=dropped)\n",
    "        test_df = test_df.drop(columns=dropped)\n",
    "    else:\n",
    "        print('No constant cols found.')\n",
    "    return train_df, test_df, dropped\n",
    "\n",
    "def scale_features(train_df, test_df, feature_cols):\n",
    "    train_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].mean())\n",
    "    test_df[feature_cols]  = test_df[feature_cols].fillna(train_df[feature_cols].mean())\n",
    "    scaler = StandardScaler()\n",
    "    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "    test_df[feature_cols]  = scaler.transform(test_df[feature_cols])\n",
    "    return train_df, test_df, scaler\n",
    "\n",
    "def create_sequences_with_units(df, feature_cols, target_col, window_size=30):\n",
    "    X,y,units = [],[],[]\n",
    "    for unit in sorted(df['unit_nr'].unique()):\n",
    "        udf = df[df['unit_nr']==unit].sort_values('time_cycles')\n",
    "        feats = udf[feature_cols].values\n",
    "        tgts  = udf[target_col].values\n",
    "        L = len(udf)\n",
    "        if L >= window_size:\n",
    "            for i in range(L-window_size+1):\n",
    "                seq = feats[i:i+window_size]\n",
    "                if np.isnan(seq).any(): continue\n",
    "                X.append(seq); y.append(tgts[i+window_size-1]); units.append(unit)\n",
    "    return np.array(X), np.array(y), np.array(units)\n",
    "\n",
    "def create_last_window_test(test_df, rul_df, feature_cols, window_size=30):\n",
    "    X_test, y_test, units = [], [], []\n",
    "    for i, unit in enumerate(sorted(test_df['unit_nr'].unique())):\n",
    "        udf = test_df[test_df['unit_nr']==unit].sort_values('time_cycles')\n",
    "        seq = udf[feature_cols].values\n",
    "        if len(seq) < window_size:\n",
    "            pad = np.zeros((window_size - len(seq), seq.shape[1]))\n",
    "            seq = np.vstack([pad, seq])\n",
    "        else:\n",
    "            seq = seq[-window_size:]\n",
    "        X_test.append(seq); y_test.append(rul_df.iloc[i,0]); units.append(unit)\n",
    "    return np.array(X_test), np.array(y_test), np.array(units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034dc02b",
   "metadata": {},
   "source": [
    "## 4) Modelos: GRU, LSTM y RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19f33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "    def __init__(self, rnn_layer, input_dim, hidden_dims=(256,128), dropout=0.3, num_layers=1):\n",
    "        super().__init__()\n",
    "        # usamos num_layers=1 para cada bloque y dropout solo si num_layers>1\n",
    "        self.rnn1 = rnn_layer(input_dim, hidden_dims[0], batch_first=True, dropout=0.0, num_layers=num_layers)\n",
    "        self.rnn2 = rnn_layer(hidden_dims[0], hidden_dims[1], batch_first=True, dropout=0.0, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(hidden_dims[1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out,_ = self.rnn1(x)\n",
    "        out,_ = self.rnn2(out)\n",
    "        out = out[:, -1, :]\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=256, hidden_dim2=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim1, num_layers=2, dropout=dropout, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim1, hidden_dim2, num_layers=2, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru1(x)\n",
    "        out, _ = self.gru2(out)\n",
    "        out = out[:, -1, :]\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(BaseRNN):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__(nn.LSTM, input_dim, hidden_dims=(256,128), dropout=0.3)\n",
    "\n",
    "class RNNModel(BaseRNN):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__(nn.RNN, input_dim, hidden_dims=(256,128), dropout=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20967f",
   "metadata": {},
   "source": [
    "## 5) Funciones entrenamiento y evaluación\n",
    "\n",
    "Funciones para entrenar una época, evaluar y entrenar con early stopping y ReduceLROnPlateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6caf1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for bx, by in dataloader:\n",
    "        bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(bx)\n",
    "        loss = criterion(out, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * bx.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def eval_model(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for bx, by in dataloader:\n",
    "            bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "            out = model(bx)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            trues.append(by.cpu().numpy())\n",
    "    if not preds:\n",
    "        return np.inf, np.inf, np.inf\n",
    "    preds = np.vstack(preds); trues = np.vstack(trues)\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, mae, rmse\n",
    "\n",
    "def fit_model_with_earlystop(model, train_loader, val_loader, n_epochs=200, lr=5e-4, weight_decay=1e-5, patience=20):\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "    best_val = np.inf; best_state = None; history = []; wait = 0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = train_one_epoch(model, optimizer, criterion, train_loader)\n",
    "        val_mse, val_mae, val_rmse = eval_model(model, criterion, val_loader)\n",
    "        history.append([epoch, train_loss, val_mse, val_mae, val_rmse])\n",
    "        scheduler.step(val_mse)\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if epoch%10==0 or epoch==1:\n",
    "            print(f'Epoch {epoch} | train {train_loss:.4f} | val_mse {val_mse:.4f} | val_mae {val_mae:.4f}')\n",
    "        if wait >= patience:\n",
    "            print('Early stopping, best val_mse=', best_val)\n",
    "            break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k:v.to(DEVICE) for k,v in best_state.items()})\n",
    "    return model, pd.DataFrame(history, columns=['epoch','train_loss','val_mse','val_mae','val_rmse'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca18b7",
   "metadata": {},
   "source": [
    "## 6) Preprocesado, CV por grupos, entrenamiento y guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563683c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FD001 =====\n",
      "=== Nulos train ===\n",
      "Series([], dtype: int64)\n",
      "=== Nulos test ===\n",
      "Series([], dtype: int64)\n",
      "Unidades test: 100, RUL rows: 100\n",
      "Filas train: 20631, filas test: 13096\n",
      "Dropped const cols: ['setting_3', 's_1', 's_5', 's_10', 's_16', 's_18', 's_19']\n",
      "Sequences: 15731\n",
      "\n",
      "--- CV GRU ---\n",
      "Fold 1\n",
      "Epoch 1 | train 7667.3068 | val_mse 6677.7910 | val_mae 61.6084\n",
      "Epoch 10 | train 3215.7516 | val_mse 3590.5166 | val_mae 47.4265\n",
      "Epoch 20 | train 198.3703 | val_mse 1220.5138 | val_mae 21.7096\n",
      "Epoch 30 | train 22.0121 | val_mse 1344.8929 | val_mae 22.4476\n",
      "Epoch 40 | train 7.4457 | val_mse 1370.7122 | val_mae 22.6359\n",
      "Early stopping, best val_mse= 988.5764\n",
      "Fold 2\n",
      "Epoch 1 | train 7833.0091 | val_mse 6421.8447 | val_mae 61.2197\n",
      "Epoch 10 | train 3275.8841 | val_mse 3350.5437 | val_mae 46.5198\n",
      "Epoch 20 | train 521.9882 | val_mse 756.5701 | val_mae 16.9415\n",
      "Epoch 30 | train 189.8829 | val_mse 781.6598 | val_mae 16.8592\n",
      "Epoch 40 | train 40.4509 | val_mse 880.2838 | val_mae 17.6964\n",
      "Early stopping, best val_mse= 740.8552\n",
      "Fold 3\n",
      "Epoch 1 | train 7829.8169 | val_mse 6309.8145 | val_mae 60.8021\n",
      "Epoch 10 | train 3288.1595 | val_mse 3301.2065 | val_mae 46.4354\n",
      "Epoch 20 | train 497.9268 | val_mse 697.5261 | val_mae 17.0717\n",
      "Epoch 30 | train 82.8357 | val_mse 1034.3627 | val_mae 20.0660\n",
      "Epoch 40 | train 15.7691 | val_mse 1157.8168 | val_mae 20.6793\n",
      "Early stopping, best val_mse= 660.1527\n",
      "Fold 4\n",
      "Epoch 1 | train 7885.8343 | val_mse 6129.2451 | val_mae 60.4528\n",
      "Epoch 10 | train 3326.3502 | val_mse 3148.8506 | val_mae 45.9370\n",
      "Epoch 20 | train 476.7977 | val_mse 627.6916 | val_mae 16.4160\n",
      "Epoch 30 | train 85.5607 | val_mse 920.2584 | val_mae 18.8182\n",
      "Epoch 40 | train 30.2189 | val_mse 1045.6842 | val_mae 19.7664\n",
      "Early stopping, best val_mse= 611.279\n",
      "Fold 5\n",
      "Epoch 1 | train 7925.6210 | val_mse 6012.2515 | val_mae 60.1656\n",
      "Epoch 10 | train 3348.1078 | val_mse 3062.2842 | val_mae 45.6091\n",
      "Epoch 20 | train 234.6783 | val_mse 800.6757 | val_mae 19.5140\n",
      "Epoch 30 | train 32.0223 | val_mse 789.0032 | val_mae 19.5653\n",
      "Epoch 40 | train 11.2425 | val_mse 801.3612 | val_mae 19.9789\n",
      "Early stopping, best val_mse= 619.41034\n",
      "\n",
      "--- CV LSTM ---\n",
      "Fold 1\n",
      "Epoch 1 | train 7769.9754 | val_mse 6751.1772 | val_mae 62.0064\n",
      "Epoch 10 | train 2115.2925 | val_mse 1682.8701 | val_mae 23.5152\n",
      "Epoch 20 | train 295.8645 | val_mse 901.5117 | val_mae 17.9831\n",
      "Epoch 30 | train 83.9290 | val_mse 928.0767 | val_mae 18.6564\n",
      "Epoch 40 | train 7.6853 | val_mse 1020.3667 | val_mae 19.1576\n",
      "Early stopping, best val_mse= 766.7076\n",
      "Fold 2\n",
      "Epoch 1 | train 7843.5851 | val_mse 6372.4604 | val_mae 60.4010\n",
      "Epoch 10 | train 659.2605 | val_mse 896.5917 | val_mae 17.7416\n",
      "Epoch 20 | train 81.7467 | val_mse 1090.6176 | val_mae 18.8962\n",
      "Epoch 30 | train 11.8177 | val_mse 1006.8286 | val_mae 19.2809\n",
      "Epoch 40 | train 3.3266 | val_mse 1011.9390 | val_mae 19.1786\n",
      "Early stopping, best val_mse= 803.7972\n",
      "Fold 3\n",
      "Epoch 1 | train 7830.7757 | val_mse 6289.0439 | val_mae 60.4888\n",
      "Epoch 10 | train 755.7011 | val_mse 795.3196 | val_mae 18.3699\n",
      "Epoch 20 | train 249.1416 | val_mse 740.1003 | val_mae 16.9566\n",
      "Epoch 30 | train 122.3680 | val_mse 837.2648 | val_mae 17.6385\n",
      "Epoch 40 | train 8.1707 | val_mse 908.1526 | val_mae 18.2497\n",
      "Early stopping, best val_mse= 740.1003\n",
      "Fold 4\n",
      "Epoch 1 | train 7904.0127 | val_mse 6196.8955 | val_mae 60.7076\n",
      "Epoch 10 | train 686.1220 | val_mse 745.5089 | val_mae 17.7634\n",
      "Epoch 20 | train 81.2993 | val_mse 1049.9238 | val_mae 20.4007\n",
      "Epoch 30 | train 37.1258 | val_mse 1117.5077 | val_mae 20.6226\n",
      "Early stopping, best val_mse= 673.7631\n",
      "Fold 5\n",
      "Epoch 1 | train 7897.1306 | val_mse 5963.0073 | val_mae 59.8876\n",
      "Epoch 10 | train 1036.5232 | val_mse 724.6153 | val_mae 17.0944\n",
      "Epoch 20 | train 296.0428 | val_mse 697.4579 | val_mae 18.1030\n",
      "Epoch 30 | train 84.2338 | val_mse 676.8691 | val_mae 17.6168\n",
      "Early stopping, best val_mse= 598.9029\n",
      "\n",
      "--- CV RNN ---\n",
      "Fold 1\n",
      "Epoch 1 | train 7841.2344 | val_mse 6867.0474 | val_mae 63.2640\n",
      "Epoch 10 | train 2396.4849 | val_mse 2421.4570 | val_mae 33.6611\n",
      "Epoch 20 | train 1266.9163 | val_mse 2005.1022 | val_mae 30.0278\n",
      "Epoch 30 | train 995.9636 | val_mse 1408.9091 | val_mae 24.6467\n",
      "Epoch 40 | train 965.1774 | val_mse 1426.1393 | val_mae 24.5437\n",
      "Epoch 50 | train 801.4273 | val_mse 1331.0774 | val_mae 23.6686\n",
      "Epoch 60 | train 878.7984 | val_mse 1363.4906 | val_mae 24.0255\n",
      "Epoch 70 | train 812.1535 | val_mse 1283.5049 | val_mae 22.9919\n",
      "Epoch 80 | train 744.5696 | val_mse 1083.9741 | val_mae 21.6504\n",
      "Epoch 90 | train 573.3969 | val_mse 1308.9614 | val_mae 21.6945\n",
      "Epoch 100 | train 612.2929 | val_mse 1334.2975 | val_mae 21.7851\n",
      "Epoch 110 | train 526.3609 | val_mse 1324.5314 | val_mae 21.8783\n",
      "Epoch 120 | train 435.7493 | val_mse 1364.8293 | val_mae 22.1429\n",
      "Early stopping, best val_mse= 931.4021\n",
      "Fold 2\n",
      "Epoch 1 | train 7927.2701 | val_mse 6546.4370 | val_mae 62.2603\n",
      "Epoch 10 | train 4785.3860 | val_mse 5544.3408 | val_mae 58.0464\n",
      "Epoch 20 | train 2982.4512 | val_mse 3343.1230 | val_mae 40.6709\n",
      "Epoch 30 | train 1919.9749 | val_mse 2157.6179 | val_mae 31.1057\n",
      "Epoch 40 | train 1319.3280 | val_mse 1378.6539 | val_mae 25.5683\n",
      "Epoch 50 | train 1083.1251 | val_mse 1272.1484 | val_mae 23.7357\n",
      "Epoch 60 | train 1054.0205 | val_mse 1223.2726 | val_mae 24.4041\n",
      "Epoch 70 | train 1050.4570 | val_mse 1292.2474 | val_mae 24.4643\n",
      "Epoch 80 | train 993.9821 | val_mse 1319.1360 | val_mae 24.6268\n",
      "Epoch 90 | train 987.3885 | val_mse 1133.4980 | val_mae 22.0941\n",
      "Epoch 100 | train 932.8253 | val_mse 1051.9384 | val_mae 21.3131\n",
      "Epoch 110 | train 890.7943 | val_mse 1009.1895 | val_mae 21.6163\n",
      "Epoch 120 | train 914.5581 | val_mse 1134.1125 | val_mae 22.4755\n",
      "Epoch 130 | train 861.3746 | val_mse 1160.4819 | val_mae 22.9412\n",
      "Early stopping, best val_mse= 1008.16174\n",
      "Fold 3\n",
      "Epoch 1 | train 7879.3271 | val_mse 6399.8652 | val_mae 61.2956\n",
      "Epoch 10 | train 5550.1101 | val_mse 2356.3208 | val_mae 34.1158\n",
      "Epoch 20 | train 1026.6663 | val_mse 923.4371 | val_mae 20.3978\n",
      "Epoch 30 | train 1040.9876 | val_mse 906.8284 | val_mae 20.1580\n",
      "Epoch 40 | train 990.8145 | val_mse 1025.4393 | val_mae 21.3039\n",
      "Epoch 50 | train 874.4691 | val_mse 895.1910 | val_mae 19.9355\n",
      "Epoch 60 | train 845.5957 | val_mse 826.1602 | val_mae 19.0692\n",
      "Epoch 70 | train 609.5134 | val_mse 1123.3342 | val_mae 22.1559\n",
      "Epoch 80 | train 479.5190 | val_mse 820.8924 | val_mae 18.2272\n",
      "Epoch 90 | train 442.6517 | val_mse 766.2960 | val_mae 18.0194\n",
      "Epoch 100 | train 373.1198 | val_mse 828.8337 | val_mae 18.8585\n",
      "Early stopping, best val_mse= 622.1616\n",
      "Fold 4\n",
      "Epoch 1 | train 7887.8679 | val_mse 6140.9927 | val_mae 60.5192\n",
      "Epoch 10 | train 9492.1191 | val_mse 7854.3276 | val_mae 73.6699\n",
      "Epoch 20 | train 3983.8385 | val_mse 2829.6453 | val_mae 40.2406\n",
      "Epoch 30 | train 1016.7853 | val_mse 1286.2336 | val_mae 25.1284\n",
      "Epoch 40 | train 1026.6521 | val_mse 1375.4623 | val_mae 27.5722\n",
      "Epoch 50 | train 879.7801 | val_mse 1234.4688 | val_mae 24.6777\n",
      "Early stopping, best val_mse= 1107.5315\n",
      "Fold 5\n",
      "Epoch 1 | train 7881.0700 | val_mse 5992.1772 | val_mae 60.0624\n",
      "Epoch 10 | train 6334.5918 | val_mse 5548.1841 | val_mae 56.8286\n",
      "Epoch 20 | train 1268.8137 | val_mse 954.3876 | val_mae 20.5528\n",
      "Epoch 30 | train 1189.0691 | val_mse 769.5511 | val_mae 18.6885\n",
      "Epoch 40 | train 1109.5097 | val_mse 832.6328 | val_mae 20.5369\n",
      "Epoch 50 | train 1023.7449 | val_mse 740.8538 | val_mae 18.8690\n",
      "Epoch 60 | train 818.5832 | val_mse 777.3245 | val_mae 19.5171\n",
      "Early stopping, best val_mse= 694.63666\n",
      "\n",
      "FINAL TRAIN GRU\n",
      "Epoch 1 | train 7486.5550 | val_mse 5787.2920 | val_mae 57.9526\n",
      "Epoch 10 | train 638.8860 | val_mse 576.5375 | val_mae 13.9927\n",
      "Epoch 20 | train 48.0889 | val_mse 42.2272 | val_mae 3.7051\n",
      "Epoch 30 | train 10.0485 | val_mse 8.1529 | val_mae 1.8672\n",
      "Epoch 40 | train 4.3933 | val_mse 2.4786 | val_mae 1.1037\n",
      "Epoch 50 | train 2.7203 | val_mse 1.8537 | val_mae 1.0252\n",
      "Epoch 60 | train 2.1220 | val_mse 0.9963 | val_mae 0.7447\n",
      "Epoch 70 | train 1.4752 | val_mse 1.0126 | val_mae 0.7993\n",
      "Epoch 80 | train 1.0759 | val_mse 0.6237 | val_mae 0.6013\n",
      "Epoch 90 | train 0.8512 | val_mse 0.5823 | val_mae 0.5858\n",
      "Epoch 100 | train 0.7326 | val_mse 0.5088 | val_mae 0.5455\n",
      "Epoch 110 | train 0.5879 | val_mse 0.5702 | val_mae 0.5890\n",
      "Epoch 120 | train 0.4879 | val_mse 0.2283 | val_mae 0.3783\n",
      "Epoch 130 | train 0.4619 | val_mse 0.2674 | val_mae 0.3969\n",
      "Epoch 140 | train 0.3323 | val_mse 0.2425 | val_mae 0.3872\n",
      "Epoch 150 | train 0.3010 | val_mse 0.1523 | val_mae 0.3071\n",
      "Epoch 160 | train 0.2299 | val_mse 0.1898 | val_mae 0.3625\n",
      "Epoch 170 | train 0.2174 | val_mse 0.0968 | val_mae 0.2473\n",
      "Epoch 180 | train 0.2135 | val_mse 0.1161 | val_mae 0.2767\n",
      "Epoch 190 | train 0.1842 | val_mse 0.0756 | val_mae 0.2197\n",
      "Epoch 200 | train 0.1666 | val_mse 0.0704 | val_mae 0.2115\n",
      "TEST GRU mse 641.0249 mae 18.541557 rmse 25.31847\n",
      "\n",
      "FINAL TRAIN LSTM\n",
      "Epoch 1 | train 7501.0503 | val_mse 5763.1021 | val_mae 57.8203\n",
      "Epoch 10 | train 528.5828 | val_mse 458.2267 | val_mae 11.9423\n",
      "Epoch 20 | train 87.9478 | val_mse 34.1650 | val_mae 3.2950\n",
      "Epoch 30 | train 7.2596 | val_mse 5.9288 | val_mae 1.5479\n",
      "Epoch 40 | train 3.2131 | val_mse 2.6623 | val_mae 1.1956\n",
      "Epoch 50 | train 1.7925 | val_mse 2.0884 | val_mae 1.0833\n",
      "Epoch 60 | train 35.6812 | val_mse 15.6794 | val_mae 2.7697\n",
      "Epoch 70 | train 0.8759 | val_mse 0.7665 | val_mae 0.6403\n",
      "Epoch 80 | train 0.6134 | val_mse 0.6856 | val_mae 0.6296\n",
      "Epoch 90 | train 0.5061 | val_mse 0.3597 | val_mae 0.4621\n",
      "Epoch 100 | train 0.3810 | val_mse 0.2790 | val_mae 0.4022\n",
      "Epoch 110 | train 0.3517 | val_mse 0.2875 | val_mae 0.4072\n",
      "Epoch 120 | train 0.0912 | val_mse 0.0929 | val_mae 0.2356\n",
      "Epoch 130 | train 0.0509 | val_mse 0.0384 | val_mae 0.1543\n",
      "Epoch 140 | train 0.0476 | val_mse 0.0422 | val_mae 0.1599\n",
      "Epoch 150 | train 0.0278 | val_mse 0.0291 | val_mae 0.1340\n",
      "Epoch 160 | train 0.0258 | val_mse 0.0239 | val_mae 0.1217\n",
      "Epoch 170 | train 0.0240 | val_mse 0.0208 | val_mae 0.1124\n",
      "Epoch 180 | train 0.0225 | val_mse 0.0192 | val_mae 0.1088\n",
      "Epoch 190 | train 0.0213 | val_mse 0.0186 | val_mae 0.1076\n",
      "Epoch 200 | train 0.0202 | val_mse 0.0202 | val_mae 0.1116\n",
      "TEST LSTM mse 724.48724 mae 17.72176 rmse 26.9163\n",
      "\n",
      "FINAL TRAIN RNN\n",
      "Epoch 1 | train 7511.7296 | val_mse 5820.5864 | val_mae 58.1370\n",
      "Epoch 10 | train 1065.4010 | val_mse 940.5074 | val_mae 20.2049\n",
      "Epoch 20 | train 909.2175 | val_mse 885.9657 | val_mae 19.8552\n",
      "Epoch 30 | train 1018.2990 | val_mse 1002.3409 | val_mae 20.9649\n",
      "Epoch 40 | train 977.6326 | val_mse 995.4683 | val_mae 21.2298\n",
      "Early stopping, best val_mse= 871.10754\n",
      "TEST RNN mse 408.2911 mae 14.358103 rmse 20.206215\n",
      "\n",
      "===== FD002 =====\n",
      "=== Nulos train ===\n",
      "Series([], dtype: int64)\n",
      "=== Nulos test ===\n",
      "Series([], dtype: int64)\n",
      "Unidades test: 259, RUL rows: 259\n",
      "Filas train: 53759, filas test: 33991\n",
      "No constant cols found.\n",
      "Sequences: 41019\n",
      "\n",
      "--- CV GRU ---\n",
      "Fold 1\n",
      "Epoch 1 | train 6073.5893 | val_mse 4108.6001 | val_mae 48.9666\n",
      "Epoch 10 | train 3311.2340 | val_mse 3393.8745 | val_mae 46.7880\n",
      "Epoch 20 | train 67.3009 | val_mse 1443.7153 | val_mae 25.6640\n",
      "Epoch 30 | train 8.4453 | val_mse 1561.6588 | val_mae 27.1548\n",
      "Early stopping, best val_mse= 1141.1013\n",
      "Fold 2\n",
      "Epoch 1 | train 6095.1678 | val_mse 4057.6794 | val_mae 48.8733\n",
      "Epoch 10 | train 3208.1432 | val_mse 2796.2908 | val_mae 41.9062\n",
      "Epoch 20 | train 651.9209 | val_mse 1079.8297 | val_mae 22.4723\n",
      "Epoch 30 | train 56.6813 | val_mse 1197.5421 | val_mae 23.0629\n",
      "Epoch 40 | train 17.4637 | val_mse 1236.9071 | val_mae 23.5543\n",
      "Early stopping, best val_mse= 819.3997\n",
      "Fold 3\n",
      "Epoch 1 | train 6148.2357 | val_mse 4000.8203 | val_mae 48.7580\n",
      "Epoch 10 | train 1079.8924 | val_mse 1002.4084 | val_mae 21.2434\n",
      "Epoch 20 | train 65.4511 | val_mse 1258.6135 | val_mae 24.4473\n",
      "Epoch 30 | train 12.1797 | val_mse 1243.0604 | val_mae 24.7367\n",
      "Early stopping, best val_mse= 980.3527\n",
      "Fold 4\n",
      "Epoch 1 | train 6160.4642 | val_mse 4012.5364 | val_mae 48.8097\n",
      "Epoch 10 | train 3321.2010 | val_mse 3241.0730 | val_mae 46.1201\n",
      "Epoch 20 | train 255.4716 | val_mse 1142.0754 | val_mae 22.8918\n",
      "Epoch 30 | train 19.4531 | val_mse 1409.1565 | val_mae 26.2817\n",
      "Epoch 40 | train 7.2464 | val_mse 1465.4045 | val_mae 26.9979\n",
      "Early stopping, best val_mse= 936.96234\n",
      "Fold 5\n",
      "Epoch 1 | train 6150.5354 | val_mse 4048.8188 | val_mae 48.8860\n",
      "Epoch 10 | train 3327.8299 | val_mse 3327.3682 | val_mae 46.7270\n",
      "Epoch 20 | train 689.2221 | val_mse 956.2255 | val_mae 20.5320\n",
      "Epoch 30 | train 55.4383 | val_mse 1365.4471 | val_mae 24.5109\n",
      "Epoch 40 | train 14.3695 | val_mse 1449.9437 | val_mae 25.4641\n",
      "Early stopping, best val_mse= 956.22546\n",
      "\n",
      "--- CV LSTM ---\n",
      "Fold 1\n",
      "Epoch 1 | train 6117.6708 | val_mse 4119.5796 | val_mae 49.0182\n",
      "Epoch 10 | train 1265.9514 | val_mse 1277.2572 | val_mae 25.5440\n",
      "Epoch 20 | train 168.1440 | val_mse 1408.2208 | val_mae 25.6752\n",
      "Epoch 30 | train 20.3275 | val_mse 1275.8329 | val_mae 23.8313\n",
      "Early stopping, best val_mse= 1006.30164\n",
      "Fold 2\n",
      "Epoch 1 | train 6227.5879 | val_mse 4114.1836 | val_mae 49.1397\n",
      "Epoch 10 | train 2925.5083 | val_mse 1862.8282 | val_mae 29.7400\n",
      "Epoch 20 | train 725.9768 | val_mse 1095.3365 | val_mae 22.9063\n",
      "Epoch 30 | train 89.7037 | val_mse 1147.9360 | val_mae 22.0056\n",
      "Epoch 40 | train 27.4019 | val_mse 1100.4124 | val_mae 22.2508\n",
      "Early stopping, best val_mse= 913.24274\n",
      "Fold 3\n",
      "Epoch 1 | train 6167.9500 | val_mse 4014.8684 | val_mae 48.8239\n",
      "Epoch 10 | train 3339.4030 | val_mse 3284.9741 | val_mae 46.5321\n",
      "Epoch 20 | train 852.3964 | val_mse 898.0283 | val_mae 19.8109\n",
      "Epoch 30 | train 597.1686 | val_mse 1004.0042 | val_mae 21.1799\n",
      "Epoch 40 | train 287.5745 | val_mse 1270.8055 | val_mae 23.9008\n",
      "Early stopping, best val_mse= 873.1692\n",
      "Fold 4\n",
      "Epoch 1 | train 6192.7902 | val_mse 4029.2678 | val_mae 48.8887\n",
      "Epoch 10 | train 399.3624 | val_mse 1346.9333 | val_mae 26.2500\n",
      "Epoch 20 | train 8.8983 | val_mse 1556.9512 | val_mae 27.8928\n",
      "Epoch 30 | train 5.1121 | val_mse 1586.4390 | val_mae 28.4118\n",
      "Early stopping, best val_mse= 1240.5944\n",
      "Fold 5\n",
      "Epoch 1 | train 6088.6040 | val_mse 4017.2764 | val_mae 48.7368\n",
      "Epoch 10 | train 1249.2793 | val_mse 1504.6554 | val_mae 28.6747\n",
      "Epoch 20 | train 36.9996 | val_mse 1500.6337 | val_mae 27.1424\n",
      "Epoch 30 | train 6.8100 | val_mse 1497.6744 | val_mae 26.9608\n",
      "Early stopping, best val_mse= 1377.1292\n",
      "\n",
      "--- CV RNN ---\n",
      "Fold 1\n",
      "Epoch 1 | train 6217.4250 | val_mse 4174.5254 | val_mae 49.2768\n",
      "Epoch 10 | train 4053.4366 | val_mse 3396.6826 | val_mae 46.6925\n",
      "Epoch 20 | train 3288.9100 | val_mse 3393.7627 | val_mae 46.8226\n",
      "Epoch 30 | train 995.1536 | val_mse 1528.8380 | val_mae 28.7338\n",
      "Epoch 40 | train 167.7416 | val_mse 1927.3992 | val_mae 30.7711\n",
      "Epoch 50 | train 58.7821 | val_mse 1649.2284 | val_mae 29.2852\n",
      "Early stopping, best val_mse= 1512.3134\n",
      "Fold 2\n",
      "Epoch 1 | train 6199.7710 | val_mse 4111.9404 | val_mae 49.1290\n",
      "Epoch 10 | train 3323.1343 | val_mse 3345.7939 | val_mae 46.6644\n",
      "Epoch 20 | train 3322.4715 | val_mse 3343.1758 | val_mae 46.6829\n",
      "Epoch 30 | train 2864.1808 | val_mse 2414.9202 | val_mae 37.5673\n",
      "Epoch 40 | train 1375.3596 | val_mse 1065.3577 | val_mae 23.3291\n",
      "Epoch 50 | train 1168.2240 | val_mse 1014.3373 | val_mae 22.8386\n",
      "Epoch 60 | train 1017.6081 | val_mse 956.7072 | val_mae 21.0182\n",
      "Epoch 70 | train 959.1211 | val_mse 878.0566 | val_mae 20.7293\n",
      "Epoch 80 | train 918.9963 | val_mse 776.2133 | val_mae 18.8797\n",
      "Epoch 90 | train 843.4620 | val_mse 818.4985 | val_mae 19.4890\n",
      "Early stopping, best val_mse= 747.8681\n",
      "Fold 3\n",
      "Epoch 1 | train 6201.8710 | val_mse 4034.5601 | val_mae 48.9178\n",
      "Epoch 10 | train 3493.9199 | val_mse 3285.1296 | val_mae 46.4851\n",
      "Epoch 20 | train 1608.9072 | val_mse 1521.9039 | val_mae 27.8940\n",
      "Epoch 30 | train 1225.1938 | val_mse 1243.7484 | val_mae 24.3348\n",
      "Epoch 40 | train 777.3127 | val_mse 1210.1661 | val_mae 24.9134\n",
      "Epoch 50 | train 329.4678 | val_mse 1376.1240 | val_mae 25.9469\n",
      "Early stopping, best val_mse= 1134.8663\n",
      "Fold 4\n",
      "Epoch 1 | train 6162.5531 | val_mse 4005.1677 | val_mae 48.7752\n",
      "Epoch 10 | train 3338.4833 | val_mse 3285.6533 | val_mae 46.5478\n",
      "Epoch 20 | train 1690.0420 | val_mse 1663.3074 | val_mae 31.5284\n",
      "Epoch 30 | train 1289.4415 | val_mse 1277.7274 | val_mae 26.9926\n",
      "Epoch 40 | train 1203.0313 | val_mse 1250.8937 | val_mae 26.6711\n",
      "Epoch 50 | train 1054.0733 | val_mse 1210.0432 | val_mae 25.0446\n",
      "Epoch 60 | train 810.7965 | val_mse 1243.1418 | val_mae 25.7594\n",
      "Epoch 70 | train 617.2488 | val_mse 1219.4742 | val_mae 25.0514\n",
      "Early stopping, best val_mse= 1143.1187\n",
      "Fold 5\n",
      "Epoch 1 | train 6227.6637 | val_mse 4097.0962 | val_mae 49.1143\n",
      "Epoch 10 | train 3669.2608 | val_mse 3327.7603 | val_mae 46.6484\n",
      "Epoch 20 | train 3531.4671 | val_mse 3499.9739 | val_mae 47.3211\n",
      "Epoch 30 | train 3350.8758 | val_mse 3328.4629 | val_mae 46.6128\n",
      "Epoch 40 | train 1438.4049 | val_mse 1590.4618 | val_mae 28.6625\n",
      "Epoch 50 | train 1204.1304 | val_mse 1508.7196 | val_mae 28.2270\n",
      "Epoch 60 | train 1172.1821 | val_mse 1495.3445 | val_mae 28.1588\n",
      "Epoch 70 | train 1121.4495 | val_mse 1456.3525 | val_mae 27.9727\n",
      "Epoch 80 | train 1108.6756 | val_mse 1437.0278 | val_mae 27.7051\n",
      "Epoch 90 | train 1113.3196 | val_mse 1406.6561 | val_mae 27.3826\n",
      "Epoch 100 | train 1103.1310 | val_mse 1443.5002 | val_mae 27.5629\n",
      "Epoch 110 | train 1023.8122 | val_mse 1351.0044 | val_mae 26.8578\n",
      "Epoch 120 | train 981.5858 | val_mse 1285.8959 | val_mae 25.8996\n",
      "Epoch 130 | train 924.6512 | val_mse 1266.1399 | val_mae 25.8676\n",
      "Epoch 140 | train 877.0019 | val_mse 1131.8300 | val_mae 23.8832\n",
      "Epoch 150 | train 847.5896 | val_mse 1176.1637 | val_mae 25.1100\n",
      "Epoch 160 | train 824.4029 | val_mse 1073.3947 | val_mae 22.6069\n",
      "Epoch 170 | train 808.4240 | val_mse 1075.9899 | val_mae 22.8785\n",
      "Epoch 180 | train 780.9582 | val_mse 1093.2443 | val_mae 21.9941\n",
      "Epoch 190 | train 765.8535 | val_mse 1093.7428 | val_mae 21.9433\n",
      "Epoch 200 | train 752.0641 | val_mse 1025.3113 | val_mae 21.6398\n",
      "\n",
      "FINAL TRAIN GRU\n",
      "Epoch 1 | train 5699.9926 | val_mse 3684.6304 | val_mae 47.2621\n",
      "Epoch 10 | train 84.2890 | val_mse 84.6724 | val_mae 6.9243\n",
      "Epoch 20 | train 15.4013 | val_mse 7.0646 | val_mae 1.8580\n",
      "Epoch 30 | train 8.2625 | val_mse 3.4462 | val_mae 1.4068\n",
      "Epoch 40 | train 4.7990 | val_mse 3.3244 | val_mae 1.2328\n",
      "Epoch 50 | train 3.8923 | val_mse 1.4370 | val_mae 0.9210\n",
      "Epoch 60 | train 2.7808 | val_mse 1.2734 | val_mae 0.8612\n",
      "Epoch 70 | train 3.1994 | val_mse 1.5418 | val_mae 0.9710\n",
      "Epoch 80 | train 4.4278 | val_mse 1.4861 | val_mae 0.9401\n",
      "Epoch 90 | train 4.8621 | val_mse 1.9463 | val_mae 1.0771\n",
      "Epoch 100 | train 8.3504 | val_mse 2.5976 | val_mae 1.2293\n",
      "Epoch 110 | train 4.4215 | val_mse 1.6894 | val_mae 0.9805\n",
      "Epoch 120 | train 1.7216 | val_mse 0.7765 | val_mae 0.6781\n",
      "Epoch 130 | train 1.0671 | val_mse 0.3891 | val_mae 0.4787\n",
      "Epoch 140 | train 2.1347 | val_mse 3.5521 | val_mae 1.3297\n",
      "Epoch 150 | train 2.9740 | val_mse 3.9824 | val_mae 0.8922\n",
      "Epoch 160 | train 1.7572 | val_mse 7.9935 | val_mae 1.6494\n",
      "Epoch 170 | train 0.6510 | val_mse 0.2531 | val_mae 0.3947\n",
      "Epoch 180 | train 0.4874 | val_mse 0.2104 | val_mae 0.3600\n",
      "Epoch 190 | train 0.4456 | val_mse 0.1903 | val_mae 0.3462\n",
      "Epoch 200 | train 0.3291 | val_mse 0.1088 | val_mae 0.2565\n",
      "TEST GRU mse 1359.0013 mae 26.007465 rmse 36.864635\n",
      "\n",
      "FINAL TRAIN LSTM\n",
      "Epoch 1 | train 5663.2579 | val_mse 3660.7349 | val_mae 47.1656\n",
      "Epoch 10 | train 1137.0520 | val_mse 1070.9915 | val_mae 23.7169\n",
      "Epoch 20 | train 58.7383 | val_mse 56.0279 | val_mae 5.5379\n",
      "Epoch 30 | train 18.8084 | val_mse 11.3550 | val_mae 2.4491\n",
      "Epoch 40 | train 12.2021 | val_mse 6.1533 | val_mae 1.8593\n",
      "Epoch 50 | train 4.2780 | val_mse 2.7274 | val_mae 1.1956\n",
      "Epoch 60 | train 8.3303 | val_mse 7.0424 | val_mae 1.6043\n",
      "Epoch 70 | train 5.7851 | val_mse 6.6741 | val_mae 1.4188\n",
      "Epoch 80 | train 5.2274 | val_mse 4.1199 | val_mae 1.2572\n",
      "Epoch 90 | train 10.0592 | val_mse 3.6264 | val_mae 1.2623\n",
      "Epoch 100 | train 1.6610 | val_mse 0.5470 | val_mae 0.5638\n",
      "Epoch 110 | train 7.6589 | val_mse 5.7036 | val_mae 1.4822\n",
      "Epoch 120 | train 3.1855 | val_mse 3.0618 | val_mae 0.8765\n",
      "Epoch 130 | train 0.2404 | val_mse 0.2044 | val_mae 0.3345\n",
      "Epoch 140 | train 0.0832 | val_mse 0.0745 | val_mae 0.2118\n",
      "Epoch 150 | train 0.0672 | val_mse 0.0750 | val_mae 0.2194\n",
      "Epoch 160 | train 0.0586 | val_mse 0.0544 | val_mae 0.1764\n",
      "Epoch 170 | train 0.0505 | val_mse 0.0480 | val_mae 0.1690\n",
      "Epoch 180 | train 0.0466 | val_mse 0.0390 | val_mae 0.1488\n",
      "Epoch 190 | train 0.0464 | val_mse 0.0425 | val_mae 0.1594\n",
      "Epoch 200 | train 0.0169 | val_mse 0.0175 | val_mae 0.1030\n",
      "TEST LSTM mse 1218.6613 mae 24.584599 rmse 34.90933\n",
      "\n",
      "FINAL TRAIN RNN\n",
      "Epoch 1 | train 5721.3703 | val_mse 3685.3657 | val_mae 47.2650\n",
      "Epoch 10 | train 3523.5457 | val_mse 3321.1311 | val_mae 46.7112\n",
      "Epoch 20 | train 1334.2349 | val_mse 1407.1012 | val_mae 28.2814\n",
      "Epoch 30 | train 976.3051 | val_mse 975.4773 | val_mae 22.5552\n",
      "Epoch 40 | train 944.6450 | val_mse 906.2460 | val_mae 20.7560\n",
      "Epoch 50 | train 927.9508 | val_mse 875.1395 | val_mae 20.0520\n",
      "Epoch 60 | train 918.0858 | val_mse 881.0243 | val_mae 20.3609\n",
      "Epoch 70 | train 819.5510 | val_mse 805.3332 | val_mae 18.6869\n",
      "Epoch 80 | train 806.5377 | val_mse 779.8910 | val_mae 18.8225\n",
      "Epoch 90 | train 786.6005 | val_mse 747.9695 | val_mae 18.7629\n",
      "Epoch 100 | train 771.4082 | val_mse 756.6211 | val_mae 18.4241\n",
      "Epoch 110 | train 712.0133 | val_mse 733.9972 | val_mae 18.8137\n",
      "Epoch 120 | train 702.0222 | val_mse 697.2931 | val_mae 17.6263\n",
      "Epoch 130 | train 693.2132 | val_mse 688.6530 | val_mae 17.9418\n",
      "Epoch 140 | train 681.4732 | val_mse 706.7070 | val_mae 18.1498\n",
      "Epoch 150 | train 672.0481 | val_mse 644.1265 | val_mae 16.9366\n",
      "Epoch 160 | train 663.7128 | val_mse 629.6873 | val_mae 16.9913\n",
      "Epoch 170 | train 656.9477 | val_mse 657.8923 | val_mae 17.4042\n",
      "Epoch 180 | train 646.8855 | val_mse 636.6330 | val_mae 16.8750\n",
      "Epoch 190 | train 637.1782 | val_mse 608.7060 | val_mae 16.7600\n",
      "Epoch 200 | train 629.8837 | val_mse 621.2518 | val_mae 16.6995\n",
      "TEST RNN mse 1242.2528 mae 23.133013 rmse 35.245605\n",
      "\n",
      "===== FD003 =====\n",
      "=== Nulos train ===\n",
      "Series([], dtype: int64)\n",
      "=== Nulos test ===\n",
      "Series([], dtype: int64)\n",
      "Unidades test: 100, RUL rows: 100\n",
      "Filas train: 24720, filas test: 16596\n",
      "Dropped const cols: ['setting_3', 's_1', 's_5', 's_16', 's_18', 's_19']\n",
      "Sequences: 19820\n",
      "\n",
      "--- CV GRU ---\n",
      "Fold 1\n",
      "Epoch 1 | train 17313.8036 | val_mse 15482.4229 | val_mae 89.1423\n",
      "Epoch 10 | train 5379.8899 | val_mse 4889.5903 | val_mae 41.4007\n",
      "Epoch 20 | train 500.2308 | val_mse 2481.3550 | val_mae 32.1408\n",
      "Epoch 30 | train 50.1575 | val_mse 3101.0823 | val_mae 35.4153\n",
      "Epoch 40 | train 17.6862 | val_mse 3135.6030 | val_mae 35.6381\n",
      "Early stopping, best val_mse= 2361.2756\n",
      "Fold 2\n",
      "Epoch 1 | train 17436.8442 | val_mse 14640.3008 | val_mae 87.8029\n",
      "Epoch 10 | train 2063.4473 | val_mse 2274.5774 | val_mae 28.2197\n",
      "Epoch 20 | train 183.1107 | val_mse 1861.1100 | val_mae 27.2786\n",
      "Epoch 30 | train 23.6777 | val_mse 1789.9783 | val_mae 27.4810\n",
      "Epoch 40 | train 8.6315 | val_mse 1962.5236 | val_mae 28.6888\n",
      "Early stopping, best val_mse= 1706.061\n",
      "Fold 3\n",
      "Epoch 1 | train 17567.9504 | val_mse 14620.6338 | val_mae 87.8923\n",
      "Epoch 10 | train 5548.1815 | val_mse 3936.0610 | val_mae 35.3828\n",
      "Epoch 20 | train 462.2510 | val_mse 2185.8960 | val_mae 28.3497\n",
      "Epoch 30 | train 52.3935 | val_mse 2455.5022 | val_mae 31.5269\n",
      "Epoch 40 | train 19.0343 | val_mse 2482.4973 | val_mae 31.9062\n",
      "Early stopping, best val_mse= 1983.1127\n",
      "Fold 4\n",
      "Epoch 1 | train 17588.8641 | val_mse 14581.7842 | val_mae 87.9196\n",
      "Epoch 10 | train 6856.1324 | val_mse 4652.7158 | val_mae 43.6451\n",
      "Epoch 20 | train 839.8278 | val_mse 1924.7231 | val_mae 28.3100\n",
      "Epoch 30 | train 141.3428 | val_mse 2073.5374 | val_mae 28.8750\n",
      "Epoch 40 | train 36.7610 | val_mse 1984.6272 | val_mae 27.8975\n",
      "Early stopping, best val_mse= 1763.848\n",
      "Fold 5\n",
      "Epoch 1 | train 17555.2213 | val_mse 14413.7031 | val_mae 87.5922\n",
      "Epoch 10 | train 3585.0356 | val_mse 3272.9856 | val_mae 35.8217\n",
      "Epoch 20 | train 472.4153 | val_mse 2639.4622 | val_mae 31.9710\n",
      "Epoch 30 | train 64.5290 | val_mse 2883.1318 | val_mae 32.4838\n",
      "Early stopping, best val_mse= 2118.986\n",
      "\n",
      "--- CV LSTM ---\n",
      "Fold 1\n",
      "Epoch 1 | train 17391.7389 | val_mse 15495.5918 | val_mae 89.1769\n",
      "Epoch 10 | train 2099.0474 | val_mse 3135.4241 | val_mae 34.1296\n",
      "Epoch 20 | train 323.3141 | val_mse 2560.2817 | val_mae 31.8828\n",
      "Epoch 30 | train 26.8074 | val_mse 3222.3867 | val_mae 34.2492\n",
      "Early stopping, best val_mse= 2220.7014\n",
      "Fold 2\n",
      "Epoch 1 | train 17418.8552 | val_mse 14585.9062 | val_mae 87.2078\n",
      "Epoch 10 | train 2725.2944 | val_mse 2397.8335 | val_mae 26.9430\n",
      "Epoch 20 | train 976.8069 | val_mse 1326.2095 | val_mae 22.5111\n",
      "Epoch 30 | train 426.9734 | val_mse 1235.2695 | val_mae 21.8699\n",
      "Epoch 40 | train 70.7130 | val_mse 2191.1665 | val_mae 26.8815\n",
      "Early stopping, best val_mse= 1220.8184\n",
      "Fold 3\n",
      "Epoch 1 | train 17501.6819 | val_mse 14528.1816 | val_mae 87.5319\n",
      "Epoch 10 | train 6027.4639 | val_mse 4838.5015 | val_mae 43.0001\n",
      "Epoch 20 | train 1321.3231 | val_mse 1901.6273 | val_mae 26.2928\n",
      "Epoch 30 | train 920.3189 | val_mse 2078.0723 | val_mae 27.1561\n",
      "Epoch 40 | train 1173.8907 | val_mse 1956.6252 | val_mae 26.3199\n",
      "Epoch 50 | train 594.3703 | val_mse 2006.0896 | val_mae 28.8442\n",
      "Epoch 60 | train 239.5812 | val_mse 2137.3157 | val_mae 30.3006\n",
      "Early stopping, best val_mse= 1785.1072\n",
      "Fold 4\n",
      "Epoch 1 | train 17506.9710 | val_mse 14442.3223 | val_mae 87.3762\n",
      "Epoch 10 | train 2665.9966 | val_mse 2752.0579 | val_mae 32.3622\n",
      "Epoch 20 | train 877.3065 | val_mse 2116.5190 | val_mae 31.4584\n",
      "Epoch 30 | train 276.8430 | val_mse 2265.2764 | val_mae 31.1583\n",
      "Epoch 40 | train 47.1711 | val_mse 2172.6543 | val_mae 30.5554\n",
      "Early stopping, best val_mse= 1654.1548\n",
      "Fold 5\n",
      "Epoch 1 | train 17451.6045 | val_mse 14300.0674 | val_mae 87.1498\n",
      "Epoch 10 | train 4299.1388 | val_mse 3491.8027 | val_mae 35.2780\n",
      "Epoch 20 | train 989.6430 | val_mse 1988.7017 | val_mae 26.9925\n",
      "Epoch 30 | train 350.1557 | val_mse 2269.4478 | val_mae 29.2397\n",
      "Epoch 40 | train 162.9773 | val_mse 2577.7312 | val_mae 30.5321\n",
      "Early stopping, best val_mse= 1690.0013\n",
      "\n",
      "--- CV RNN ---\n",
      "Fold 1\n",
      "Epoch 1 | train 17512.5051 | val_mse 15955.0918 | val_mae 92.5952\n",
      "Epoch 10 | train 3862.1327 | val_mse 4345.0068 | val_mae 41.7477\n",
      "Epoch 20 | train 2217.2012 | val_mse 4147.9272 | val_mae 44.6560\n",
      "Epoch 30 | train 1858.0090 | val_mse 5007.9932 | val_mae 49.2809\n",
      "Epoch 40 | train 1640.7666 | val_mse 4923.9810 | val_mae 47.8409\n",
      "Early stopping, best val_mse= 3647.91\n",
      "Fold 2\n",
      "Epoch 1 | train 17767.3937 | val_mse 15140.8555 | val_mae 91.3876\n",
      "Epoch 10 | train 4653.4548 | val_mse 3947.0742 | val_mae 37.3441\n",
      "Epoch 20 | train 2588.3056 | val_mse 2299.8481 | val_mae 31.5012\n",
      "Epoch 30 | train 2491.6667 | val_mse 2135.2556 | val_mae 30.5199\n",
      "Epoch 40 | train 2391.5619 | val_mse 2024.2722 | val_mae 29.7838\n",
      "Epoch 50 | train 2850.3875 | val_mse 2043.9976 | val_mae 30.9884\n",
      "Epoch 60 | train 2348.0278 | val_mse 2060.4209 | val_mae 29.9671\n",
      "Epoch 70 | train 2148.7423 | val_mse 1780.8716 | val_mae 28.1076\n",
      "Epoch 80 | train 2101.5879 | val_mse 1906.8909 | val_mae 29.3582\n",
      "Epoch 90 | train 1925.6940 | val_mse 2109.5557 | val_mae 30.5944\n",
      "Early stopping, best val_mse= 1630.035\n",
      "Fold 3\n",
      "Epoch 1 | train 17727.7651 | val_mse 14804.6064 | val_mae 89.2740\n",
      "Epoch 10 | train 4436.6776 | val_mse 4014.5930 | val_mae 41.1247\n",
      "Epoch 20 | train 2400.0119 | val_mse 2633.8669 | val_mae 35.0820\n",
      "Epoch 30 | train 2080.1984 | val_mse 2755.2766 | val_mae 31.7279\n",
      "Epoch 40 | train 1967.7447 | val_mse 2937.8923 | val_mae 33.2957\n",
      "Early stopping, best val_mse= 2633.867\n",
      "Fold 4\n",
      "Epoch 1 | train 17760.9143 | val_mse 14889.6113 | val_mae 91.1920\n",
      "Epoch 10 | train 3682.8275 | val_mse 3323.0283 | val_mae 36.1440\n",
      "Epoch 20 | train 2551.8575 | val_mse 2333.1504 | val_mae 33.2970\n",
      "Epoch 30 | train 2603.0415 | val_mse 3468.2268 | val_mae 38.9589\n",
      "Epoch 40 | train 2208.6979 | val_mse 2685.4824 | val_mae 36.0401\n",
      "Early stopping, best val_mse= 2170.7612\n",
      "Fold 5\n",
      "Epoch 1 | train 17741.3791 | val_mse 14602.6895 | val_mae 88.4247\n",
      "Epoch 10 | train 4428.7770 | val_mse 3787.1804 | val_mae 39.1328\n",
      "Epoch 20 | train 2369.0351 | val_mse 2661.0649 | val_mae 34.9238\n",
      "Epoch 30 | train 1674.4953 | val_mse 2904.3164 | val_mae 33.5782\n",
      "Epoch 40 | train 1629.3060 | val_mse 2656.3430 | val_mae 32.4440\n",
      "Early stopping, best val_mse= 2493.8125\n",
      "\n",
      "FINAL TRAIN GRU\n",
      "Epoch 1 | train 16838.5447 | val_mse 13694.9688 | val_mae 84.0305\n",
      "Epoch 10 | train 2078.2245 | val_mse 1820.2095 | val_mae 24.0699\n",
      "Epoch 20 | train 238.4836 | val_mse 286.3174 | val_mae 8.4724\n",
      "Epoch 30 | train 35.3113 | val_mse 26.6996 | val_mae 3.2343\n",
      "Epoch 40 | train 14.8480 | val_mse 14.0187 | val_mae 2.6697\n",
      "Epoch 50 | train 9.6803 | val_mse 5.0317 | val_mae 1.5977\n",
      "Epoch 60 | train 7.2836 | val_mse 3.8186 | val_mae 1.4718\n",
      "Epoch 70 | train 46.6458 | val_mse 5.3520 | val_mae 1.7590\n",
      "Epoch 80 | train 4.1561 | val_mse 2.7196 | val_mae 1.2658\n",
      "Epoch 90 | train 2.7295 | val_mse 1.3846 | val_mae 0.9012\n",
      "Epoch 100 | train 2.6958 | val_mse 1.6430 | val_mae 0.9959\n",
      "Epoch 110 | train 3.1151 | val_mse 1.9912 | val_mae 1.0864\n",
      "Epoch 120 | train 46.7216 | val_mse 12.7434 | val_mae 2.2584\n",
      "Epoch 130 | train 2.1727 | val_mse 1.4620 | val_mae 0.9535\n",
      "Epoch 140 | train 1.4664 | val_mse 0.7969 | val_mae 0.6912\n",
      "Epoch 150 | train 1.2181 | val_mse 0.6954 | val_mae 0.6410\n",
      "Epoch 160 | train 1.0124 | val_mse 0.6931 | val_mae 0.6507\n",
      "Epoch 170 | train 0.8602 | val_mse 0.5633 | val_mae 0.5929\n",
      "Epoch 180 | train 0.6856 | val_mse 0.3394 | val_mae 0.4500\n",
      "Epoch 190 | train 0.5713 | val_mse 0.2427 | val_mae 0.3848\n",
      "Epoch 200 | train 0.5025 | val_mse 0.2121 | val_mae 0.3615\n",
      "TEST GRU mse 1368.8319 mae 23.091902 rmse 36.99773\n",
      "\n",
      "FINAL TRAIN LSTM\n",
      "Epoch 1 | train 16870.8654 | val_mse 13685.7490 | val_mae 83.7190\n",
      "Epoch 10 | train 1765.3882 | val_mse 1698.1332 | val_mae 24.4063\n",
      "Epoch 20 | train 424.4754 | val_mse 368.3577 | val_mae 9.8856\n",
      "Epoch 30 | train 90.3728 | val_mse 71.2224 | val_mae 4.6597\n",
      "Epoch 40 | train 21.4448 | val_mse 33.7509 | val_mae 3.6981\n",
      "Epoch 50 | train 10.4474 | val_mse 8.9082 | val_mae 2.1034\n",
      "Epoch 60 | train 26.6458 | val_mse 8.3930 | val_mae 2.1113\n",
      "Epoch 70 | train 3.2990 | val_mse 3.4048 | val_mae 1.2991\n",
      "Epoch 80 | train 4.2569 | val_mse 4.6046 | val_mae 1.5449\n",
      "Epoch 90 | train 44.6297 | val_mse 236.2257 | val_mae 9.4742\n",
      "Epoch 100 | train 3.7095 | val_mse 4.2110 | val_mae 1.4725\n",
      "Epoch 110 | train 2.8191 | val_mse 3.9261 | val_mae 1.4533\n",
      "Epoch 120 | train 2.3120 | val_mse 2.8225 | val_mae 1.2083\n",
      "Epoch 130 | train 0.8111 | val_mse 0.7130 | val_mae 0.6418\n",
      "Epoch 140 | train 0.9974 | val_mse 1.0617 | val_mae 0.7620\n",
      "Epoch 150 | train 0.3913 | val_mse 0.4148 | val_mae 0.4909\n",
      "Epoch 160 | train 0.1939 | val_mse 0.1945 | val_mae 0.3416\n",
      "Epoch 170 | train 0.1714 | val_mse 0.1984 | val_mae 0.3420\n",
      "Epoch 180 | train 0.1550 | val_mse 0.1426 | val_mae 0.2919\n",
      "Epoch 190 | train 0.1445 | val_mse 0.1381 | val_mae 0.2881\n",
      "Epoch 200 | train 0.1344 | val_mse 0.1372 | val_mae 0.2843\n",
      "TEST LSTM mse 1117.1952 mae 21.754265 rmse 33.42447\n",
      "\n",
      "FINAL TRAIN RNN\n",
      "Epoch 1 | train 16972.9295 | val_mse 13885.1006 | val_mae 85.5206\n",
      "Epoch 10 | train 8054.1723 | val_mse 6649.7778 | val_mae 49.5532\n",
      "Epoch 20 | train 2711.2027 | val_mse 2765.5774 | val_mae 35.9956\n",
      "Epoch 30 | train 2619.3400 | val_mse 2535.3149 | val_mae 32.8511\n",
      "Epoch 40 | train 2311.2991 | val_mse 2340.8804 | val_mae 32.6809\n",
      "Epoch 50 | train 2370.2426 | val_mse 2362.9102 | val_mae 32.5690\n",
      "Epoch 60 | train 2227.9401 | val_mse 2243.6719 | val_mae 31.2912\n",
      "Epoch 70 | train 2168.4293 | val_mse 2149.9978 | val_mae 31.1885\n",
      "Epoch 80 | train 2148.0787 | val_mse 2172.7498 | val_mae 30.8590\n",
      "Epoch 90 | train 2106.5504 | val_mse 2201.8801 | val_mae 30.8359\n",
      "Epoch 100 | train 1970.5060 | val_mse 1882.3467 | val_mae 29.0751\n",
      "Epoch 110 | train 1984.2201 | val_mse 1993.7822 | val_mae 29.9229\n",
      "Epoch 120 | train 1859.9425 | val_mse 1812.7773 | val_mae 27.9799\n",
      "Epoch 130 | train 1561.7546 | val_mse 1542.3352 | val_mae 26.7180\n",
      "Epoch 140 | train 1464.7240 | val_mse 1439.3402 | val_mae 25.6020\n",
      "Epoch 150 | train 1589.5441 | val_mse 1588.7651 | val_mae 25.9714\n",
      "Epoch 160 | train 1429.3242 | val_mse 1448.1208 | val_mae 25.2179\n",
      "Epoch 170 | train 1298.5519 | val_mse 1275.2013 | val_mae 23.8092\n",
      "Epoch 180 | train 1374.5779 | val_mse 1322.8079 | val_mae 23.8430\n",
      "Epoch 190 | train 1296.2189 | val_mse 1309.7720 | val_mae 23.6181\n",
      "Epoch 200 | train 1277.3578 | val_mse 1212.9506 | val_mae 22.5961\n",
      "TEST RNN mse 1724.25 mae 26.013186 rmse 41.52409\n",
      "\n",
      "===== FD004 =====\n",
      "=== Nulos train ===\n",
      "Series([], dtype: int64)\n",
      "=== Nulos test ===\n",
      "Series([], dtype: int64)\n",
      "Unidades test: 248, RUL rows: 248\n",
      "Filas train: 61249, filas test: 41214\n",
      "No constant cols found.\n",
      "Sequences: 49048\n",
      "\n",
      "--- CV GRU ---\n",
      "Fold 1\n",
      "Epoch 1 | train 11442.2917 | val_mse 8690.1250 | val_mae 67.7941\n",
      "Epoch 10 | train 6150.9895 | val_mse 6925.7349 | val_mae 64.6819\n",
      "Epoch 20 | train 105.7619 | val_mse 3803.2180 | val_mae 43.5713\n",
      "Epoch 30 | train 20.2982 | val_mse 4108.9966 | val_mae 44.6333\n",
      "Early stopping, best val_mse= 3097.3945\n",
      "Fold 2\n",
      "Epoch 1 | train 11625.9753 | val_mse 8030.6929 | val_mae 66.8009\n",
      "Epoch 10 | train 2073.3016 | val_mse 2189.1970 | val_mae 31.9224\n",
      "Epoch 20 | train 56.2831 | val_mse 2665.7871 | val_mae 35.8368\n",
      "Epoch 30 | train 22.0421 | val_mse 2867.2856 | val_mae 36.8532\n",
      "Early stopping, best val_mse= 2189.197\n",
      "Fold 3\n",
      "Epoch 1 | train 11748.3291 | val_mse 7786.6460 | val_mae 66.2396\n",
      "Epoch 10 | train 6356.5520 | val_mse 6127.9331 | val_mae 63.2909\n",
      "Epoch 20 | train 160.2768 | val_mse 2922.9026 | val_mae 38.6449\n",
      "Epoch 30 | train 21.7217 | val_mse 3281.9480 | val_mae 40.9492\n",
      "Early stopping, best val_mse= 2341.6033\n",
      "Fold 4\n",
      "Epoch 1 | train 11797.4510 | val_mse 7736.1826 | val_mae 66.1540\n",
      "Epoch 10 | train 6369.8101 | val_mse 6080.9409 | val_mae 63.1367\n",
      "Epoch 20 | train 272.0063 | val_mse 2841.3040 | val_mae 36.9323\n",
      "Epoch 30 | train 31.9597 | val_mse 2872.0154 | val_mae 37.9373\n",
      "Epoch 40 | train 11.7821 | val_mse 2957.7205 | val_mae 38.6074\n",
      "Early stopping, best val_mse= 1969.6423\n",
      "Fold 5\n",
      "Epoch 1 | train 11747.5536 | val_mse 7713.4458 | val_mae 66.0817\n",
      "Epoch 10 | train 6362.4341 | val_mse 6076.5928 | val_mae 63.2384\n",
      "Epoch 20 | train 152.6167 | val_mse 2764.7151 | val_mae 36.2087\n",
      "Epoch 30 | train 22.4570 | val_mse 2852.0813 | val_mae 36.6203\n",
      "Early stopping, best val_mse= 2136.2449\n",
      "\n",
      "--- CV LSTM ---\n",
      "Fold 1\n",
      "Epoch 1 | train 11477.2843 | val_mse 8688.7520 | val_mae 67.7896\n",
      "Epoch 10 | train 2568.3510 | val_mse 3106.2234 | val_mae 37.4094\n",
      "Epoch 20 | train 277.9081 | val_mse 3971.4446 | val_mae 42.8719\n",
      "Epoch 30 | train 43.5048 | val_mse 3974.9565 | val_mae 43.0194\n",
      "Early stopping, best val_mse= 2701.8625\n",
      "Fold 2\n",
      "Epoch 1 | train 11537.6658 | val_mse 7973.3477 | val_mae 66.6042\n",
      "Epoch 10 | train 2341.0045 | val_mse 1794.4661 | val_mae 28.9501\n",
      "Epoch 20 | train 366.5319 | val_mse 2401.5176 | val_mae 34.1874\n",
      "Epoch 30 | train 84.9028 | val_mse 2573.6880 | val_mae 36.1043\n",
      "Early stopping, best val_mse= 1794.4661\n",
      "Fold 3\n",
      "Epoch 1 | train 11744.6386 | val_mse 7764.5396 | val_mae 66.1641\n",
      "Epoch 10 | train 3199.4854 | val_mse 2762.1777 | val_mae 38.2160\n",
      "Epoch 20 | train 304.1422 | val_mse 3128.2649 | val_mae 39.7291\n",
      "Epoch 30 | train 94.8999 | val_mse 3200.4780 | val_mae 39.9809\n",
      "Early stopping, best val_mse= 2305.638\n",
      "Fold 4\n",
      "Epoch 1 | train 11738.7608 | val_mse 7700.0366 | val_mae 66.0301\n",
      "Epoch 10 | train 3550.1335 | val_mse 3276.8508 | val_mae 42.8284\n",
      "Epoch 20 | train 1659.4362 | val_mse 2100.2549 | val_mae 31.8651\n",
      "Epoch 30 | train 301.2735 | val_mse 2617.8806 | val_mae 34.9821\n",
      "Epoch 40 | train 68.3694 | val_mse 2592.8020 | val_mae 34.9971\n",
      "Early stopping, best val_mse= 1966.6938\n",
      "Fold 5\n",
      "Epoch 1 | train 11669.5641 | val_mse 7657.6514 | val_mae 65.8901\n",
      "Epoch 10 | train 2656.7738 | val_mse 2421.8042 | val_mae 33.3660\n",
      "Epoch 20 | train 203.0450 | val_mse 2558.5034 | val_mae 34.3303\n",
      "Epoch 30 | train 24.0690 | val_mse 2657.5493 | val_mae 34.0436\n",
      "Early stopping, best val_mse= 2089.8337\n",
      "\n",
      "--- CV RNN ---\n",
      "Fold 1\n",
      "Epoch 1 | train 11559.2914 | val_mse 8763.4736 | val_mae 68.0365\n",
      "Epoch 10 | train 6150.9825 | val_mse 6917.9082 | val_mae 65.0098\n",
      "Epoch 20 | train 3331.3417 | val_mse 4156.8340 | val_mae 48.1049\n",
      "Epoch 30 | train 2532.1418 | val_mse 3557.0669 | val_mae 41.8399\n",
      "Epoch 40 | train 2408.7402 | val_mse 3301.0918 | val_mae 39.6058\n",
      "Epoch 50 | train 1744.7767 | val_mse 3369.4983 | val_mae 39.8648\n",
      "Epoch 60 | train 789.4695 | val_mse 3670.0134 | val_mae 41.5022\n",
      "Epoch 70 | train 491.4265 | val_mse 3954.6230 | val_mae 42.3366\n",
      "Early stopping, best val_mse= 3245.7449\n",
      "Fold 2\n",
      "Epoch 1 | train 11686.9142 | val_mse 8064.0073 | val_mae 66.9155\n",
      "Epoch 10 | train 11781.4548 | val_mse 10385.6055 | val_mae 76.1735\n",
      "Epoch 20 | train 4910.7242 | val_mse 5094.6157 | val_mae 56.7450\n",
      "Epoch 30 | train 3082.8051 | val_mse 2610.2419 | val_mae 37.1430\n",
      "Epoch 40 | train 2805.1665 | val_mse 2553.3850 | val_mae 37.2033\n",
      "Epoch 50 | train 2658.6826 | val_mse 2270.2529 | val_mae 34.0403\n",
      "Epoch 60 | train 2564.3215 | val_mse 2510.5237 | val_mae 37.4053\n",
      "Epoch 70 | train 2296.9032 | val_mse 2357.9236 | val_mae 33.9447\n",
      "Epoch 80 | train 2030.6811 | val_mse 2540.5886 | val_mae 36.1792\n",
      "Early stopping, best val_mse= 2266.8909\n",
      "Fold 3\n",
      "Epoch 1 | train 11836.5839 | val_mse 7818.5718 | val_mae 66.3485\n",
      "Epoch 10 | train 6397.2911 | val_mse 6133.9951 | val_mae 63.0987\n",
      "Epoch 20 | train 2715.7037 | val_mse 2704.1562 | val_mae 36.5933\n",
      "Epoch 30 | train 2629.2870 | val_mse 2449.6697 | val_mae 35.1697\n",
      "Epoch 40 | train 2593.6308 | val_mse 2458.4001 | val_mae 34.9377\n",
      "Epoch 50 | train 2515.1621 | val_mse 2384.9304 | val_mae 34.9805\n",
      "Epoch 60 | train 2237.4793 | val_mse 2158.6738 | val_mae 31.9684\n",
      "Epoch 70 | train 2090.1682 | val_mse 2172.4878 | val_mae 34.1192\n",
      "Epoch 80 | train 1956.4106 | val_mse 1953.8455 | val_mae 31.5887\n",
      "Epoch 90 | train 1878.0161 | val_mse 1854.5865 | val_mae 30.9563\n",
      "Epoch 100 | train 1858.6947 | val_mse 1852.4731 | val_mae 29.9473\n",
      "Epoch 110 | train 1848.2007 | val_mse 1836.4561 | val_mae 29.7714\n",
      "Epoch 120 | train 1811.0569 | val_mse 1798.6124 | val_mae 30.0104\n",
      "Epoch 130 | train 1799.2122 | val_mse 1813.4077 | val_mae 30.3721\n",
      "Epoch 140 | train 1793.0066 | val_mse 1794.5767 | val_mae 30.2038\n",
      "Epoch 150 | train 1764.1620 | val_mse 1778.9507 | val_mae 30.0214\n",
      "Epoch 160 | train 1759.5710 | val_mse 1736.0493 | val_mae 29.4330\n",
      "Epoch 170 | train 1742.7733 | val_mse 1731.6545 | val_mae 29.4611\n",
      "Epoch 180 | train 1732.2926 | val_mse 1726.8905 | val_mae 29.2434\n",
      "Epoch 190 | train 1729.2653 | val_mse 1731.7817 | val_mae 29.4372\n",
      "Epoch 200 | train 1724.8875 | val_mse 1736.6368 | val_mae 29.4781\n",
      "Fold 4\n",
      "Epoch 1 | train 11817.0401 | val_mse 7761.0342 | val_mae 66.2391\n",
      "Epoch 10 | train 6361.5199 | val_mse 6077.7070 | val_mae 63.0702\n",
      "Epoch 20 | train 2889.3173 | val_mse 2879.9546 | val_mae 39.0937\n",
      "Epoch 30 | train 6376.9913 | val_mse 6067.2144 | val_mae 63.0649\n",
      "Epoch 40 | train 2744.6297 | val_mse 2677.8889 | val_mae 39.1479\n",
      "Epoch 50 | train 2542.3203 | val_mse 2705.4155 | val_mae 39.1630\n",
      "Epoch 60 | train 2431.9808 | val_mse 2956.0532 | val_mae 39.5080\n",
      "Early stopping, best val_mse= 2637.697\n",
      "Fold 5\n",
      "Epoch 1 | train 11751.0844 | val_mse 7715.7598 | val_mae 66.0896\n",
      "Epoch 10 | train 6363.1528 | val_mse 6074.6875 | val_mae 63.0969\n",
      "Epoch 20 | train 7806.9984 | val_mse 7202.3506 | val_mae 65.0513\n",
      "Epoch 30 | train 3053.8609 | val_mse 2842.7632 | val_mae 35.9741\n",
      "Epoch 40 | train 2767.0174 | val_mse 2364.3923 | val_mae 33.0476\n",
      "Epoch 50 | train 2263.8269 | val_mse 2469.3188 | val_mae 35.2151\n",
      "Epoch 60 | train 1878.1335 | val_mse 2140.3787 | val_mae 31.4025\n",
      "Epoch 70 | train 1651.4254 | val_mse 2273.4045 | val_mae 31.6715\n",
      "Epoch 80 | train 1376.4717 | val_mse 2328.3137 | val_mae 32.6446\n",
      "Early stopping, best val_mse= 2067.3542\n",
      "\n",
      "FINAL TRAIN GRU\n",
      "Epoch 1 | train 10930.0775 | val_mse 7241.9414 | val_mae 64.2424\n",
      "Epoch 10 | train 1067.6316 | val_mse 860.9743 | val_mae 19.8729\n",
      "Epoch 20 | train 83.0859 | val_mse 58.3501 | val_mae 4.6921\n",
      "Epoch 30 | train 23.3194 | val_mse 119.7626 | val_mae 8.3719\n",
      "Epoch 40 | train 10.3754 | val_mse 10.5746 | val_mae 2.4498\n",
      "Epoch 50 | train 9.8441 | val_mse 6.3155 | val_mae 1.8112\n",
      "Epoch 60 | train 5.1297 | val_mse 3.2152 | val_mae 1.3601\n",
      "Epoch 70 | train 6.4772 | val_mse 3.4309 | val_mae 1.4342\n",
      "Epoch 80 | train 48.6580 | val_mse 18.5696 | val_mae 2.9202\n",
      "Epoch 90 | train 3.9333 | val_mse 1.7595 | val_mae 1.0297\n",
      "Epoch 100 | train 1.9268 | val_mse 1.2217 | val_mae 0.8606\n",
      "Epoch 110 | train 1.4536 | val_mse 0.7565 | val_mae 0.6766\n",
      "Epoch 120 | train 1.1688 | val_mse 0.6865 | val_mae 0.6470\n",
      "Epoch 130 | train 2.7313 | val_mse 0.8890 | val_mae 0.7079\n",
      "Epoch 140 | train 0.8313 | val_mse 0.4431 | val_mae 0.5267\n",
      "Epoch 150 | train 0.6434 | val_mse 0.2673 | val_mae 0.4037\n",
      "Epoch 160 | train 0.5343 | val_mse 0.2504 | val_mae 0.3904\n",
      "Epoch 170 | train 0.4670 | val_mse 0.1727 | val_mae 0.3227\n",
      "Epoch 180 | train 0.4498 | val_mse 0.1563 | val_mae 0.3028\n",
      "Epoch 190 | train 0.3927 | val_mse 0.1634 | val_mae 0.3132\n",
      "Epoch 200 | train 0.3399 | val_mse 0.1156 | val_mae 0.2621\n",
      "TEST GRU mse 1681.4246 mae 31.004492 rmse 41.005177\n",
      "\n",
      "FINAL TRAIN LSTM\n",
      "Epoch 1 | train 10852.7328 | val_mse 7203.4883 | val_mae 64.1350\n",
      "Epoch 10 | train 1166.5016 | val_mse 830.8973 | val_mae 18.3808\n",
      "Epoch 20 | train 100.1120 | val_mse 101.7987 | val_mae 5.8033\n",
      "Epoch 30 | train 54.5602 | val_mse 34.2015 | val_mae 3.0177\n",
      "Epoch 40 | train 58.2982 | val_mse 34.8325 | val_mae 2.6852\n",
      "Epoch 50 | train 50.7361 | val_mse 20.3033 | val_mae 2.9330\n",
      "Epoch 60 | train 36.0955 | val_mse 13.6255 | val_mae 2.0168\n",
      "Epoch 70 | train 7.2603 | val_mse 2.3628 | val_mae 0.9802\n",
      "Epoch 80 | train 2.3866 | val_mse 0.7841 | val_mae 0.6578\n",
      "Epoch 90 | train 0.8237 | val_mse 13.3725 | val_mae 0.8557\n",
      "Epoch 100 | train 3.9854 | val_mse 17.2546 | val_mae 2.2166\n",
      "Epoch 110 | train 1.5816 | val_mse 0.5926 | val_mae 0.5627\n",
      "Epoch 120 | train 4.1325 | val_mse 1.9385 | val_mae 0.8226\n",
      "Epoch 130 | train 0.5302 | val_mse 0.3690 | val_mae 0.4675\n",
      "Epoch 140 | train 0.9682 | val_mse 0.6151 | val_mae 0.4974\n",
      "Epoch 150 | train 0.3050 | val_mse 0.3591 | val_mae 0.4585\n",
      "Epoch 160 | train 1.8605 | val_mse 0.2783 | val_mae 0.4054\n",
      "Epoch 170 | train 0.2700 | val_mse 0.2335 | val_mae 0.3677\n",
      "Epoch 180 | train 0.0780 | val_mse 0.0844 | val_mae 0.2232\n",
      "Epoch 190 | train 0.0715 | val_mse 0.0636 | val_mae 0.1937\n",
      "Epoch 200 | train 0.0277 | val_mse 0.0261 | val_mae 0.1267\n",
      "TEST LSTM mse 2031.2562 mae 34.117085 rmse 45.06946\n",
      "\n",
      "FINAL TRAIN RNN\n",
      "Epoch 1 | train 10915.5021 | val_mse 7228.3979 | val_mae 64.2041\n",
      "Epoch 10 | train 6364.6181 | val_mse 4818.6924 | val_mae 52.7425\n",
      "Epoch 20 | train 2977.5872 | val_mse 2710.8547 | val_mae 38.2356\n",
      "Epoch 30 | train 2671.0413 | val_mse 2535.7925 | val_mae 35.6359\n",
      "Epoch 40 | train 2629.3678 | val_mse 2459.0530 | val_mae 34.8672\n",
      "Epoch 50 | train 2468.9956 | val_mse 2423.0034 | val_mae 34.4699\n",
      "Epoch 60 | train 2558.5025 | val_mse 2414.2258 | val_mae 34.9273\n",
      "Epoch 70 | train 2248.3283 | val_mse 5394.7583 | val_mae 60.5844\n",
      "Epoch 80 | train 2518.0672 | val_mse 1883.9507 | val_mae 31.4168\n",
      "Epoch 90 | train 4417.2964 | val_mse 3727.0295 | val_mae 46.2392\n",
      "Epoch 100 | train 2136.3908 | val_mse 2547.9084 | val_mae 37.3045\n",
      "Epoch 110 | train 1430.3500 | val_mse 1434.8165 | val_mae 28.6758\n",
      "Epoch 120 | train 899.6179 | val_mse 1540.6962 | val_mae 28.4878\n",
      "Epoch 130 | train 666.9290 | val_mse 723.7800 | val_mae 18.9286\n",
      "Epoch 140 | train 862.5817 | val_mse 663.6652 | val_mae 18.4507\n",
      "Epoch 150 | train 542.1511 | val_mse 480.7573 | val_mae 15.3695\n",
      "Epoch 160 | train 551.4814 | val_mse 393.8588 | val_mae 13.8926\n",
      "Epoch 170 | train 400.3704 | val_mse 390.2765 | val_mae 13.8357\n",
      "Epoch 180 | train 465.4036 | val_mse 352.8426 | val_mae 12.9723\n",
      "Epoch 190 | train 260.1245 | val_mse 252.7805 | val_mae 10.8052\n",
      "Epoch 200 | train 217.1901 | val_mse 298.9638 | val_mae 12.1796\n",
      "TEST RNN mse 2468.6382 mae 38.047165 rmse 49.68539\n",
      "Finalizado todo el procesamiento y entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "FD_LIST = ['FD001','FD002','FD003','FD004']\n",
    "WINDOW_SIZE = 50\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "for fd in FD_LIST:\n",
    "    print('\\n=====', fd, '=====')\n",
    "    train_df, test_df, rul_df = load_fd_dataset(fd)\n",
    "    train_df = add_rul(train_df)\n",
    "    report_file = check_and_report_raw(train_df, test_df, rul_df, fd)\n",
    "    FEATURE_COLS = ['setting_1','setting_2','setting_3'] + [f's_{i}' for i in range(1,22)]\n",
    "    train_df, test_df, dropped = remove_constant_columns(train_df, test_df, FEATURE_COLS)\n",
    "    FEATURE_COLS = [c for c in FEATURE_COLS if c not in dropped]\n",
    "    train_df, test_df, scaler = scale_features(train_df, test_df, FEATURE_COLS)\n",
    "    joblib.dump(scaler, os.path.join(MODEL_PATH, f'scaler_{fd}.pkl'))\n",
    "    X, y, units = create_sequences_with_units(train_df, FEATURE_COLS, 'RUL', WINDOW_SIZE)\n",
    "    print('Sequences:', len(X))\n",
    "    if len(X)==0:\n",
    "        print('No sequences for', fd); continue\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    algorithms = {'GRU': GRUModel, 'LSTM': LSTMModel, 'RNN': RNNModel}\n",
    "    cv_results = {alg:[] for alg in algorithms}\n",
    "    for alg_name, alg_class in algorithms.items():\n",
    "        print('\\n--- CV', alg_name, '---')\n",
    "        fold=0\n",
    "        for tr_idx, va_idx in gkf.split(X, y, groups=units):\n",
    "            fold += 1\n",
    "            print('Fold', fold)\n",
    "            Xtr = torch.tensor(X[tr_idx], dtype=torch.float32); ytr = torch.tensor(y[tr_idx], dtype=torch.float32).view(-1,1)\n",
    "            Xva = torch.tensor(X[va_idx], dtype=torch.float32); yva = torch.tensor(y[va_idx], dtype=torch.float32).view(-1,1)\n",
    "            tr_loader = DataLoader(TensorDataset(Xtr,ytr), batch_size=32, shuffle=True)\n",
    "            va_loader = DataLoader(TensorDataset(Xva,yva), batch_size=64, shuffle=False)\n",
    "            model = alg_class(input_dim=len(FEATURE_COLS))\n",
    "            model, hist = fit_model_with_earlystop(model, tr_loader, va_loader, n_epochs=200, lr=5e-4, weight_decay=1e-5, patience=25)\n",
    "            mse, mae, rmse = eval_model(model, nn.MSELoss(), va_loader)\n",
    "            hist_file = os.path.join(METRICS_PATH, f'{fd}_{alg_name}_fold{fold}_hist.csv')\n",
    "            hist.to_csv(hist_file, index=False)\n",
    "            cv_results[alg_name].append({'fold':fold,'mse':mse,'mae':mae,'rmse':rmse,'hist':hist_file})\n",
    "        pd.DataFrame(cv_results[alg_name]).to_csv(os.path.join(METRICS_PATH, f'{fd}_{alg_name}_cv_summary.csv'), index=False)\n",
    "    # Entrenamiento final con todos los datos\n",
    "    X_all = torch.tensor(X, dtype=torch.float32); y_all = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    full_loader = DataLoader(TensorDataset(X_all,y_all), batch_size=32, shuffle=True)\n",
    "    X_test, y_test, test_units = create_last_window_test(test_df, rul_df, FEATURE_COLS, WINDOW_SIZE)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32); y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1,1)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_t,y_test_t), batch_size=64, shuffle=False)\n",
    "    final_metrics = []\n",
    "    for alg_name, alg_class in algorithms.items():\n",
    "        print('\\nFINAL TRAIN', alg_name)\n",
    "        model = alg_class(input_dim=len(FEATURE_COLS))\n",
    "        model, hist = fit_model_with_earlystop(model, full_loader, full_loader, n_epochs=200, lr=5e-4, weight_decay=1e-5, patience=30)\n",
    "        mse, mae, rmse = eval_model(model, nn.MSELoss(), test_loader)\n",
    "        print('TEST', alg_name, 'mse', mse, 'mae', mae, 'rmse', rmse)\n",
    "        model_file = os.path.join(MODEL_PATH, f'{fd}_{alg_name}_final.pth')\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        preds = model(X_test_t.to(DEVICE)).cpu().detach().numpy().flatten()\n",
    "        pd.DataFrame({'unit':test_units,'RUL_true':y_test,'RUL_pred':preds}).to_csv(os.path.join(METRICS_PATH, f'{fd}_{alg_name}_preds.csv'), index=False)\n",
    "        plt.figure(figsize=(10,4)); plt.plot(y_test, label='true'); plt.plot(preds, label='pred'); plt.legend(); plt.title(f'{fd} {alg_name} test'); plt.savefig(os.path.join(FIG_PATH, f'{fd}_{alg_name}_test.png')); plt.close()\n",
    "        final_metrics.append({'alg':alg_name,'mse':mse,'mae':mae,'rmse':rmse,'model_file':model_file})\n",
    "    pd.DataFrame(final_metrics).to_csv(os.path.join(METRICS_PATH, f'{fd}_final_comparison.csv'), index=False)\n",
    "print('Finalizado todo el procesamiento y entrenamiento.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
