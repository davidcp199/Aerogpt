{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9e0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\anaconda3\\envs\\ml_python\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage\n",
    "from typing import TypedDict\n",
    "\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import pandas as pd\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import requests\n",
    "import sys\n",
    "# Añade la carpeta src al path\n",
    "sys.path.append(r\"C:\\Users\\David\\Documents\\Master-Big-Data-Data-Sciencee-e-Inteligencia-Artificial\\TFM\\AeroGPT\\src\")\n",
    "\n",
    "# Importa la función desde el archivo Predictor_RUL.py\n",
    "from Predictor_RUL import predict_RUL\n",
    "# Carga las variables de entorno desde el archivo .env\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Recupera la clave API de OpenAI desde las variables de entorno\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verifica que la clave API se haya cargado correctamente\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"La clave API de OpenAI no está configurada correctamente.\")\n",
    "\n",
    "# Ahora puedes usar la clave API en LangChain o directamente con OpenAI\n",
    "llm_decisions = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4o-mini\"   # soporta tool-calls modernas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78282138",
   "metadata": {},
   "source": [
    "STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf0ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    decision: Optional[Literal['extract', 'none']] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64466112",
   "metadata": {},
   "source": [
    "NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097a043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Generar el prompt del asistente conversacional\n",
    "PROMPT_SUPERVISOR = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Eres un supervisor cuyo objetivo es decidir si un mensaje del usuario está relacionado con:\n",
    "\n",
    "*Extracción de datos sobre motores aeronáuticos (CMAPSS)*  \n",
    "*Predicción de RUL*  \n",
    "*Sensores de motores turbofan*  \n",
    "*Datos de ciclos, settings, condiciones operativas, o sensores FI, HPC, LPT, etc.*\n",
    "\n",
    "Reglas:\n",
    "- Si el mensaje del usuario **está relacionado con CMAPSS, motores, sensores o RUL**, responde **solo** con:\n",
    "    extract\n",
    "- Si el mensaje **NO está relacionado**, responde **solo** con:\n",
    "    none\n",
    "- No agregues explicaciones, frases adicionales ni símbolos.\n",
    "\n",
    "Mensaje del usuario: {user_message}\n",
    "\n",
    "Tu respuesta: (extract/none)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "PROMTP_EXTRACT_CMAPSS = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "   Eres un asistente especializado en extraer datos estructurados para alimentar un modelo de predicción RUL basado en CMAPSS.\n",
    "\n",
    "   TU TAREA:\n",
    "   Extraer únicamente la información explícita mencionada por el usuario sobre el estado actual de un motor aeronáutico.\n",
    "\n",
    "   NO DEBES inventar valores.  \n",
    "   NO estimes sensores no mencionados.  \n",
    "   NO rellenes medias ni interpolaciones: eso lo hará el modelo después.\n",
    "\n",
    "   ------------------------------------------------------------\n",
    "   DATOS QUE DEBES EXTRAER\n",
    "   ------------------------------------------------------------\n",
    "\n",
    "   1. unidad  \n",
    "      - Identificador del motor (si no se menciona → 000)\n",
    "\n",
    "   2. tiempo_ciclos  \n",
    "      - Ciclo operativo actual (si no se menciona → 000)\n",
    "\n",
    "   3. configuraciones_operativas  \n",
    "      - Tres valores: setting_1, setting_2, setting_3  \n",
    "      - Si el usuario no menciona alguno → 000\n",
    "\n",
    "   4. mediciones_sensores  \n",
    "      - Lista EXACTA de 21 sensores (s_1 a s_21)  \n",
    "      - Si el usuario menciona un sensor (“sensor 7: 550”) asigna ese valor.  \n",
    "      - Si NO lo menciona → 000.\n",
    "\n",
    "   IMPORTANTE:\n",
    "   - NO inventes datos.\n",
    "   - NO rellenes con medias.\n",
    "   - NO derives valores no mencionados.\n",
    "\n",
    "   ------------------------------------------------------------\n",
    "   SELECCIÓN DEL MODELO (FD)\n",
    "   ------------------------------------------------------------\n",
    "\n",
    "   Selecciona el modelo usando estas reglas:\n",
    "\n",
    "   - FD001 → condiciones de nivel del mar + solo HPC degradation\n",
    "   - FD002 → condiciones SEIS + solo HPC\n",
    "   - FD003 → nivel del mar + HPC y/o Fan degradation\n",
    "   - FD004 → condiciones SEIS + HPC y/o Fan degradation\n",
    "\n",
    "   Reglas de selección:\n",
    "   - Si se menciona “nivel del mar”, “sea level” → FD001 o FD003\n",
    "   - Si se mencionan múltiples condiciones, ambiente variable, altitud variable → FD002 o FD004\n",
    "   - Si se menciona “fan”, “fan degradation”, “fan speed issues” → usar FD003 o FD004\n",
    "   - Si solo se menciona HPC degradation → usar FD001 o FD002\n",
    "   - Si no hay contexto → seleccionar FD001\n",
    "\n",
    "   ------------------------------------------------------------\n",
    "   FORMATO DE RESPUESTA (estricto JSON)\n",
    "   ------------------------------------------------------------\n",
    "\n",
    "   {{\n",
    "   \"unidad\": <int|000>,\n",
    "   \"tiempo_ciclos\": <int|000>,\n",
    "   \"configuraciones_operativas\": [setting_1, setting_2, setting_3],\n",
    "   \"mediciones_sensores\": {{\n",
    "         \"s_1\": <float|000>,\n",
    "         \"s_2\": <float|000>,\n",
    "         ...\n",
    "         \"s_21\": <float|000>\n",
    "   }},\n",
    "   \"modelo_seleccionado\": \"FD001\" | \"FD002\" | \"FD003\" | \"FD004\"\n",
    "   }}\n",
    "\n",
    "   ------------------------------------------------------------\n",
    "   MENSAJE DEL USUARIO:\n",
    "   ------------------------------------------------------------\n",
    "   {message}\n",
    "   \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05182b5",
   "metadata": {},
   "source": [
    "TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c9102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_cmapss_tool(message: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa esta herramienta SOLO cuando el supervisor decida 'extract'.\n",
    "    Extrae datos CMAPSS desde un texto del usuario.\n",
    "    \"\"\"\n",
    "    chain = PROMTP_EXTRACT_CMAPSS | llm_decisions\n",
    "    response = chain.invoke({\"message\": message})\n",
    "\n",
    "    # Limpiar output para que sea JSON válido\n",
    "    tool_output = response.content\n",
    "    # Quitar backticks si existen\n",
    "    tool_output = tool_output.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    # Reemplazar 000 por 0\n",
    "    tool_output = tool_output.replace(\": 000\", \": 0\")\n",
    "\n",
    "    print(f\"TOOL: {tool_output}\")\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def tool_output_to_df(tool_output: dict) -> pd.DataFrame:\n",
    "    \"\"\"Extrae configuraciones CMAPSS y las convierte en diccionario plano\"\"\"\n",
    "    settings = tool_output.get(\"configuraciones_operativas\", [0,0,0])\n",
    "    sensor_vals = tool_output.get(\"mediciones_sensores\", {f\"s_{i}\": 0 for i in range(1,22)})\n",
    "\n",
    "    # Creamos diccionario plano con nombres que espera el modelo\n",
    "    data = {\n",
    "        \"setting_1\": [settings[0]],\n",
    "        \"setting_2\": [settings[1]],\n",
    "        \"setting_3\": [settings[2]],\n",
    "        **{k: [v] for k, v in sensor_vals.items()}\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25412fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_action(state: AgentState):\n",
    "    \"\"\"Decide si una pregunta del usuario es una consulta de CMAPSS o si no tiene que ver con esto.\"\"\"\n",
    "    \n",
    "    \n",
    "    chain = PROMPT_SUPERVISOR | llm_decisions\n",
    "    response = chain.invoke({\"user_message\": state.messages[-1].content})\n",
    "    \n",
    "    decision = response.content.strip()\n",
    "    \n",
    "    print(f\">>> Supervisor: {decision}\")\n",
    "    return {'decision': decision}\n",
    "\n",
    "def extract_cmapss_action(state: AgentState):\n",
    "    \"\"\"Extrae datos CMAPSS y predice RUL usando el modelo GRU.\"\"\"\n",
    "    last_user_msg = state.messages[-1].content\n",
    "\n",
    "    # Ejecuta la tool\n",
    "    tool_result_str = extract_cmapss_tool.run(last_user_msg)\n",
    "    tool_result = json.loads(tool_result_str)\n",
    "\n",
    "    # Convertir a DataFrame pasando dict en el campo esperado\n",
    "    df_user = tool_output_to_df.run({\"tool_output\": tool_result})\n",
    "\n",
    "    # Predicción RUL\n",
    "    base_path = r\"C:\\Users\\David\\Documents\\Master-Big-Data-Data-Sciencee-e-Inteligencia-Artificial\\TFM\\AeroGPT\\data\\CMAPSS\"\n",
    "    rul_pred = predict_RUL(df_user, base_path, fd=tool_result[\"modelo_seleccionado\"])\n",
    "\n",
    "    print(f\">>> RUL predicho: {rul_pred['predicted_RUL']}\")\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=json.dumps(rul_pred))]}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb793d",
   "metadata": {},
   "source": [
    "Routine Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a6d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_decision(state: AgentState):\n",
    "    if state.decision == 'extract':\n",
    "        return \"Extractor\"\n",
    "    else:\n",
    "        return END\n",
    "    \n",
    "# def extract_tool_decision(state: AgentState):\n",
    "#     \"\"\"Si ya se ejecutó la tool, termina.\"\"\"\n",
    "#     ai_msg = state.messages[-1]\n",
    "#     if hasattr(ai_msg, \"tool_calls\") and ai_msg.tool_calls:\n",
    "#         return END\n",
    "#     return END\n",
    "\n",
    "def extract_tool_decision(state: AgentState):\n",
    "    \"\"\"Después de ejecutar la tool, termina el flujo.\"\"\"\n",
    "    return END\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4692391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph():\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    graph.add_node(\"Supervisor\", supervisor_action)\n",
    "    graph.add_node(\"Extractor\", extract_cmapss_action)\n",
    "\n",
    "    graph.add_edge(START, \"Supervisor\")\n",
    "    graph.add_conditional_edges(\"Supervisor\", supervisor_decision)\n",
    "    graph.add_conditional_edges(\"Extractor\", extract_tool_decision)\n",
    "\n",
    "    return graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6520de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Supervisor: extract\n",
      "TOOL: \n",
      "{\n",
      "   \"unidad\": 1,\n",
      "   \"tiempo_ciclos\": 0,\n",
      "   \"configuraciones_operativas\": [2, 3, 4],\n",
      "   \"mediciones_sensores\": {\n",
      "         \"s_1\": 100,\n",
      "         \"s_2\": 100,\n",
      "         \"s_3\": 100,\n",
      "         \"s_4\": 100,\n",
      "         \"s_5\": 100,\n",
      "         \"s_6\": 100,\n",
      "         \"s_7\": 100,\n",
      "         \"s_8\": 100,\n",
      "         \"s_9\": 100,\n",
      "         \"s_10\": 100,\n",
      "         \"s_11\": 100,\n",
      "         \"s_12\": 100,\n",
      "         \"s_13\": 100,\n",
      "         \"s_14\": 100,\n",
      "         \"s_15\": 100,\n",
      "         \"s_16\": 100,\n",
      "         \"s_17\": 100,\n",
      "         \"s_18\": 100,\n",
      "         \"s_19\": 100,\n",
      "         \"s_20\": 100,\n",
      "         \"s_21\": 100\n",
      "   },\n",
      "   \"modelo_seleccionado\": \"FD001\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Documents\\Master-Big-Data-Data-Sciencee-e-Inteligencia-Artificial\\TFM\\AeroGPT\\src\\Predictor_RUL.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> RUL predicho: 141.95713806152344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\anaconda3\\envs\\ml_python\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Supervisor: extract\n",
      "TOOL: \n",
      "{\n",
      "   \"unidad\": 1,\n",
      "   \"tiempo_ciclos\": 0,\n",
      "   \"configuraciones_operativas\": [2, 3, 4],\n",
      "   \"mediciones_sensores\": {\n",
      "         \"s_1\": 100,\n",
      "         \"s_2\": 100,\n",
      "         \"s_3\": 100,\n",
      "         \"s_4\": 100,\n",
      "         \"s_5\": 100,\n",
      "         \"s_6\": 100,\n",
      "         \"s_7\": 100,\n",
      "         \"s_8\": 100,\n",
      "         \"s_9\": 100,\n",
      "         \"s_10\": 100,\n",
      "         \"s_11\": 100,\n",
      "         \"s_12\": 100,\n",
      "         \"s_13\": 100,\n",
      "         \"s_14\": 100,\n",
      "         \"s_15\": 100,\n",
      "         \"s_16\": 100,\n",
      "         \"s_17\": 100,\n",
      "         \"s_18\": 100,\n",
      "         \"s_19\": 100,\n",
      "         \"s_20\": 100,\n",
      "         \"s_21\": 100\n",
      "   },\n",
      "   \"modelo_seleccionado\": \"FD004\"\n",
      "}\n",
      "\n",
      ">>> RUL predicho: 65.17757415771484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Documents\\Master-Big-Data-Data-Sciencee-e-Inteligencia-Artificial\\TFM\\AeroGPT\\src\\Predictor_RUL.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "c:\\Users\\David\\anaconda3\\envs\\ml_python\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "graph = update_graph()\n",
    "\n",
    "user_input = input(\"\\nPregunta del usuario: ('stop' para parar)\")\n",
    "while user_input != 'stop':\n",
    "    messages = graph.invoke(\n",
    "        input={\"messages\": [HumanMessage(content=f\"{user_input}\")]}\n",
    "    )\n",
    "    user_input = input(\"\\nPregunta del usuario: ('stop' para parar)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
